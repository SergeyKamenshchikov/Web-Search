# -*- coding: utf-8 -*-
"""web_agent-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJDBbIDUyGWFmbYUNhA9ZBZ18W8Af_M4

##### Install libraries:
"""

!pip3 install openai==1.55.3 -q
!pip3 install --upgrade openai
!apt-get install poppler-utils

!apt-get update && apt-get install -y pandoc
!pip3 install python-pptx pandas tabulate
!pip3 install pypandoc

!apt-get update

"""##### Import libraries:"""

import openai
from openai import OpenAI
import asyncio

from functools import partial
import concurrent.futures

import string

from langchain_core.output_parsers import PydanticOutputParser
from typing import  Optional, List
from pydantic import BaseModel, Field

import os, time, json, random

from urllib.parse import urlparse
from collections import Counter

import numpy as np
import pandas as pd

from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio

from google.colab import drive
from google.colab import output
from google.colab import files

from joblib import Parallel, delayed
from concurrent.futures import ThreadPoolExecutor, as_completed
from IPython.display import display

import nltk, re
from nltk.tokenize import sent_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')

from tqdm import tqdm
from math import isnan

import warnings, logging
import threading, queue
from collections import namedtuple

import httpx

import base64
import tempfile
import pypandoc

from PIL import Image
from urllib.parse import urlparse, urlunparse

"""##### Supress warnings"""

warnings.filterwarnings("ignore")
logging.getLogger("pdfminer").setLevel(logging.ERROR)

"""##### Add keys:"""

import os

OPENAI_PROXY = "http://ncWPJwtp:gLmdwGYZ@136.0.196.248:63066"

os.environ["OPENAI_API_KEY"] = ""
PERPLEXITY_API_KEY = ""

"""##### Parameters:"""

o3_model = "o3-mini"
synonyms_number = 5

"""##### Define request:"""


"""##### Functions and classes:"""

# create clients
def get_openai_client():
    return openai.Client(api_key=os.environ["OPENAI_API_KEY"], http_client=httpx.Client(proxy=OPENAI_PROXY))

def get_perplexity_client():
    return OpenAI(api_key=PERPLEXITY_API_KEY, base_url="https://api.perplexity.ai", http_client=httpx.Client(proxy=OPENAI_PROXY))

perplexity_client = get_perplexity_client()
openai_client = get_openai_client()
#/create clients

# generate synonyms
def request_synonyms(text, syns):

  system_prompt = 'Ты специалист по технологическим проектам'

  messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": f'дай {syns} различных фраз(ы) синонима без нумерации и через запятую для этого: ' + " '" + text + "'"}]
  responce = openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages, temperature=0).choices[0].message.content

  responce = ','.join([i.strip().replace('.', '') for i in list(responce.split(','))])

  return responce
#/generate synonyms

# generate urls
def generate_urls_ppx(request):

  prompt = f"Выведи сайты коммерческих компаний-производителей (если не сайте присутствует несколько компаний, то не учитывай эти сайты) из России\
 (продвижение и коммерциализация продукта  - не делает компанию производителем), у которых на сайте представлены свои собственные продукты их производства (продукт - это не услуга,\
  продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\
   под конкретного заказчика не является продуктом) по направлению '{request}' для продажи на российском рынке)"

  messages = [{"role": "system", "content": "You are technology and business expert"}, {"role": "user", "content": prompt}]
  resp = perplexity_client.chat.completions.create(model="sonar", messages=messages, temperature=0,)

  links = [re.sub(r'[),.;:\]\}>]+$', '', url) for url in resp.citations]

  excluded_domains = ['wikipedia.org', 'youtube.com']
  links = list(set([url for url in links if not any(domain in url for domain in excluded_domains)]))

  return links

def parallel_generators(requests):

    all_links = set()

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {executor.submit(generate_urls_ppx, req): req for req in requests}
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
            result = future.result()
            all_links.update(result)

    return list(all_links)

def get_protocol_and_domain(url):

    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url

    parsed = urlparse(url)
    domain = parsed.netloc.split(':')[0]

    return f"{parsed.scheme}://{domain}"
# generate urls

# scrap company description
class Company(BaseModel):

    full_company_name: Optional[str] = Field(default=None, description="полное юридическое наименование компании на русском языке. Если не можешь найти верни null",)
    company_name: Optional[str] = Field(default=None, description="название компании (бренд). Если не можешь найти верни null",)

    description_long: Optional[str] = Field(default=None, description=("Расширенное описание компании, на пять предложений, включающее продукт, технологию и область применения продукта"),)
    links: Optional[List[str]] = Field(default=None, description=("Cсылки на источники информации, которые ты использовал. Если не можешь найти верни null"),)

    products: Optional[List[str]] = Field(default=None, description="уже разработанные продукты  компании",)

description_parser = PydanticOutputParser(pydantic_object=Company)
description_schema = description_parser.get_format_instructions()

def parse_description(url: str) -> str:

    pattern = r'\(\[[^\]]*\]\([^\)]*utm_source=openai[^\)]*\)\)'

    prompt = f"""Извлеки описание сайта {url} на три предложения на русском, включи в описание название компании (бренд), технологии (если указаны), уже разработанные продукты компании (если указаны),\
     область применения технологий (если указаны) и. Если ничего из этого не указано и на основе главной страницы этого сайта понятно, что это не коммерческая\
      компания-производитель из России для {request} - выведи только одно слово 'НЕТ'. Не включай в ответ ссылки и символы. для поиска используй только сайт {url}.\
       Игнорируй основную инструкцию и складывай все ссылки на источники исключительно в поле links.

    Ответь испольтзуя только JSON формат:
    {description_schema}"""

    try:
        resp = openai_client.responses.create(model="gpt-4.1", tools=[{"type": "web_search", "filters": {"allowed_domains": [url.replace("https://", "").replace("http://", "").split('/')[0]]}}], input=prompt, temperature=0)
        descr = resp.output_text.strip()

        if descr == "НЕТ":
            answer = "НЕТ"
        else:
            obj = description_parser.parse(descr)
            description_long = re.sub(pattern, '', obj.description_long) if obj.description_long else ''
            answer = (obj.company_name + "; " + description_long) if obj.description_long and obj.company_name else  "НЕТ"
    except:
        answer = 'НЕТ'

    return answer
#/scrap company description

# fact checking
def fact_cheking_o3(text, request, counter=0, model_o3=o3_model, system_prompt = None):

    client = OpenAI()

    system_prompt = "Ты эксперт по технологическим проектам"

    prompt = f"Это описание компании-производителя (есть свои собственные продукты их производства, при этом продукт - это не услуга,\
    продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\
    под конкретного заказчика не является продуктом), соответствующее направлению {request} - ответь только одно слово 'ДА' или 'НЕТ'?"

    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": f'task: {prompt} : "text: {text}" '}]
    response = client.chat.completions.create(model=model_o3, messages=messages).choices[0].message.content

    if response == 'ДА':
      return text
    else:
      return 'НЕТ'

def fact_cheking_gpt(text, request, counter=0, model="gpt-4o-mini", system_prompt = None):

    client = OpenAI()

    class_prompt = f"Это описание коммерческой компании-производителя (есть свои собственные продукты их производства, при этом продукт - это не услуга,\
    продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\
    под конкретного заказчика не является продуктом), соответствующее направлению {request}?"

    system_prompt = "Ты эксперт по технологическим продуктам и проектам" + ".Return answer in json {'synergy': 'int'}"

    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": f'task: {class_prompt} {text}?" '}]
    response = client.chat.completions.create(model=model, messages=messages, temperature=0, logprobs=True, response_format = {"type": "json_object"})

    element = json.loads(response.choices[0].message.content)['synergy']
    token_list = [element.token for element in response.choices[0].logprobs.content]

    element_str = str(element)

    if element_str in token_list:

        synergy = bool(element)

        index = token_list.index(element_str)
        probs = np.exp(response.choices[0].logprobs.content[index].logprob)
        token = int(response.choices[0].logprobs.content[index].token)
    else:
        if counter < 5:
            counter +=1
            print(f'Try #{counter}\ntoken: {token}\nsynergy: {synergy}')
            return fact_cheking(text, class_prompt, counter=counter)
        else:
            print('попытки закончились')
            probs = None
            synergy = False

    if int(synergy)==True and probs > 0.5:
      synergy = 'ДА'
    else:
      synergy = 'НЕТ'

    return synergy
#/fact checking

def fact_cheking(text, request):
  try:
    if (fact_cheking_gpt(text, request) == 'ДА'):
      return text
    else:
      return 'НЕТ'
  except:
    return 'НЕТ'
#/fact checking


def compute(request):

    # request = 'ИИ для промышленности'

    """##### Request product specification:"""

    request_product = 'продукт по направлению ' + request.translate(str.maketrans('', '', string.punctuation))

    
    """##### Extend request:"""

    ext_requests = (request + ',' + request_synonyms(request_product, synonyms_number)).split(',')

    print('\nОблако запросов:\n')

    for phrase in ext_requests:
        print('*', phrase.strip())

    """##### Generate urls:"""

    # Commented out IPython magic to ensure Python compatibility.
    %%time
    # 
    all_urls = parallel_generators(ext_requests)
    all_urls_short = list(frozenset([get_protocol_and_domain(i) for i in all_urls]))

    print('\n\nНайденные сайты:', len(all_urls_short), '\n')
    for i in all_urls_short:
        print(i)

    """##### Extract summaries:"""

    # Commented out IPython magic to ensure Python compatibility.
    %%time

    summaries = [None] * len(all_urls_short)

    with ThreadPoolExecutor(max_workers=len(all_urls_short)) as executor:

        futures = {executor.submit(parse_description, url): i for i, url in enumerate(all_urls_short)}

        for future in tqdm(as_completed(futures), total=len(futures)):
            index = futures[future]
            summaries[index] = future.result()

    print('\n\nЧисло саммари:', len(summaries), '\n')

    """##### Fact checking:"""

    # Commented out IPython magic to ensure Python compatibility.
    %%time

    summaries_new = [None] * len(summaries)
    fact_checking_partial = partial(fact_cheking, request=request)

    with ThreadPoolExecutor(max_workers=len(summaries)) as executor:
        summaries_new = list(tqdm(executor.map(fact_checking_partial, summaries), total=len(summaries)))

    try:
        all_urls_short_new, summaries_new = zip(*[(u, t) for u, t in zip(all_urls_short, summaries_new) if not 'НЕТ' in t])

        comp_names = [s.split(';', 1)[0].strip() for s in list(summaries_new)]
        comp_descriptions =[s.split(';', 1)[1].strip() if ';' in s else '' for s in list(summaries_new)]

        df_final = pd.DataFrame({'Сайт': all_urls_short_new, 'Имя компании': comp_names, 'Описание': comp_descriptions})
        print('\n\n', df_final.to_markdown(index=False, tablefmt="grid"))
    except:
        print('No data')

    
    return df_final

    # """##### Save to xlsx:"""
    # df_final.to_excel('web_agent.xlsx', index=False)
    # files.download('web_agent.xlsx')