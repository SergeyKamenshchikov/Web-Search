{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeyKamenshchikov/Web-Search/blob/main/web_agent_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEK-UAOKmAQi"
      },
      "source": [
        "##### Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEL4Wobu_bT",
        "outputId": "12c9d02a-2d3d-4ba5-cd74-d07b226dbd8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/389.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.55.3)\n",
            "Collecting openai\n",
            "  Downloading openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.6.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.55.3\n",
            "    Uninstalling openai-1.55.3:\n",
            "      Successfully uninstalled openai-1.55.3\n",
            "Successfully installed openai-2.6.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.11 [186 kB]\n",
            "Fetched 186 kB in 0s (2,029 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.11_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,086 kB]\n",
            "Get:12 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,865 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,472 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,050 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,798 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,817 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,389 kB]\n",
            "Fetched 36.8 MB in 3s (14.7 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "pandoc set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.9 python-pptx-1.0.2\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openai==1.55.3 -q\n",
        "!pip3 install --upgrade openai\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "!apt-get update && apt-get install -y pandoc\n",
        "!pip3 install python-pptx pandas tabulate\n",
        "!pip3 install pypandoc\n",
        "\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA2KiyRanzyA"
      },
      "source": [
        "##### Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5amVqeZImFIO",
        "outputId": "1126d770-195e-4c6b-d2e4-da5c488c6fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import asyncio\n",
        "\n",
        "from functools import partial\n",
        "import concurrent.futures\n",
        "\n",
        "import string\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from typing import  Optional, List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os, time, json, random\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from IPython.display import display\n",
        "\n",
        "import nltk, re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from tqdm import tqdm\n",
        "from math import isnan\n",
        "\n",
        "import warnings, logging\n",
        "import threading, queue\n",
        "from collections import namedtuple\n",
        "\n",
        "import httpx\n",
        "\n",
        "import base64\n",
        "import tempfile\n",
        "import pypandoc\n",
        "\n",
        "from PIL import Image\n",
        "from urllib.parse import urlparse, urlunparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewazALFKnQ5j"
      },
      "source": [
        "##### Supress warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P83qiq4GnTWw"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDPp-BH0UcC"
      },
      "source": [
        "##### Add keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "fEXkJYYE0WiC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "OPENAI_PROXY = \"http://ncWPJwtp:gLmdwGYZ@136.0.196.248:63066\"\n",
        "PERPLEXITY_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameters:"
      ],
      "metadata": {
        "id": "_WidlKjKAJ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o3_model = \"o3-mini\"\n",
        "synonyms_number = 5"
      ],
      "metadata": {
        "id": "C6VXNWfUAMjX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define request:"
      ],
      "metadata": {
        "id": "HyNowqOSHcYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request = 'графен'\n",
        "request = 'продукт по направлению ' + request.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "_d-23pDiHbcy"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMvuvqKk0cFg"
      },
      "source": [
        "##### Functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create clients\n",
        "def get_openai_client():\n",
        "    return openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"], http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "def get_perplexity_client():\n",
        "    return OpenAI(api_key=PERPLEXITY_API_KEY, base_url=\"https://api.perplexity.ai\", http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "perplexity_client = get_perplexity_client()\n",
        "openai_client = get_openai_client()\n",
        "#/create clients\n",
        "\n",
        "# generate synonyms\n",
        "def request_synonyms(text, syns):\n",
        "\n",
        "  system_prompt = 'Ты специалист по технологическим проектам'\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'дай {syns} различных фраз(ы) синонима без нумерации и через запятую для этого: ' + \" '\" + text + \"'\"}]\n",
        "  responce = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=0).choices[0].message.content\n",
        "\n",
        "  responce = ','.join([i.strip().replace('.', '') for i in list(responce.split(','))])\n",
        "\n",
        "  return responce\n",
        "#/generate synonyms\n",
        "\n",
        "# generate urls\n",
        "def generate_urls_ppx(request):\n",
        "\n",
        "  prompt = f\"Выведи сайты коммерческих компаний-производителей (если не сайте присутствует несколько компаний, то не учитывай эти сайты) из России\\\n",
        " (продвижение и коммерциализация продукта  - не делает компанию производителем), у которых на сайте представлены свои собственные продукты их производства (продукт - это не услуга,\\\n",
        "  продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "   под конкретного заказчика не является продуктом) по направлению '{request}' для продажи на российском рынке)\"\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": \"You are technology and business expert\"}, {\"role\": \"user\", \"content\": prompt}]\n",
        "  resp = perplexity_client.chat.completions.create(model=\"sonar\", messages=messages, temperature=0,)\n",
        "\n",
        "  links = [re.sub(r'[),.;:\\]\\}>]+$', '', url) for url in resp.citations]\n",
        "  links = list(set([url for url in links if \"wikipedia.org\" not in url]))\n",
        "\n",
        "  return links\n",
        "\n",
        "def parallel_generators(requests):\n",
        "\n",
        "    all_links = set()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = {executor.submit(generate_urls_ppx, req): req for req in requests}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
        "            result = future.result()\n",
        "            all_links.update(result)\n",
        "\n",
        "    return list(all_links)\n",
        "\n",
        "def get_protocol_and_domain(url):\n",
        "\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'http://' + url\n",
        "\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.split(':')[0]\n",
        "\n",
        "    return f\"{parsed.scheme}://{domain}\"\n",
        "# generate urls\n",
        "\n",
        "# scrap company description\n",
        "class Company(BaseModel):\n",
        "\n",
        "    full_company_name: Optional[str] = Field(default=None, description=\"полное юридическое наименование компании на русском языке. Если не можешь найти верни null\",)\n",
        "    company_name: Optional[str] = Field(default=None, description=\"название компании (бренд). Если не можешь найти верни null\",)\n",
        "\n",
        "    description_long: Optional[str] = Field(default=None, description=(\"Расширенное описание компании, на пять предложений, включающее продукт, технологию и область применения продукта\"),)\n",
        "    links: Optional[List[str]] = Field(default=None, description=(\"Cсылки на источники информации, которые ты использовал. Если не можешь найти верни null\"),)\n",
        "\n",
        "    products: Optional[List[str]] = Field(default=None, description=\"уже разработанные продукты  компании\",)\n",
        "\n",
        "description_parser = PydanticOutputParser(pydantic_object=Company)\n",
        "description_schema = description_parser.get_format_instructions()\n",
        "\n",
        "def parse_description(url: str) -> str:\n",
        "\n",
        "    pattern = r'\\(\\[[^\\]]*\\]\\([^\\)]*utm_source=openai[^\\)]*\\)\\)'\n",
        "\n",
        "    prompt = f\"\"\"Извлеки описание сайта {url} на три предложения на русском, включи в описание название компании (бренд), технологии (если указаны), уже разработанные продукты компании (если указаны),\\\n",
        "     область применения технологий (если указаны). Если ничего из этого не указано и на основе главной страницы этого сайта понятно, что это не коммерческая\\\n",
        "      компания-производитель из России для {request} - выведи только одно слово 'НЕТ'. Не включай в ответ ссылки и символы. для поиска используй только сайт {url}.\\\n",
        "       Игнорируй основную инструкцию и складывай все ссылки на источники исключительно в поле links.\n",
        "\n",
        "    Ответь испольтзуя только JSON формат:\n",
        "    {description_schema}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = openai_client.responses.create(model=\"gpt-4.1\", tools=[{\"type\": \"web_search\", \"filters\": {\"allowed_domains\": [url.replace(\"https://\", \"\").replace(\"http://\", \"\").split('/')[0]]}}], input=prompt, temperature=0)\n",
        "        descr = resp.output_text.strip()\n",
        "\n",
        "        if descr == \"НЕТ\":\n",
        "            answer = \"НЕТ\"\n",
        "        else:\n",
        "            obj = description_parser.parse(descr)\n",
        "            description_long = re.sub(pattern, '', obj.description_long) if obj.description_long else ''\n",
        "            answer = (obj.company_name + \"; \" + description_long) if obj.description_long and obj.company_name else  \"НЕТ\"\n",
        "    except:\n",
        "        answer = 'НЕТ'\n",
        "\n",
        "    return answer\n",
        "#/scrap company description\n",
        "\n",
        "# fact checking\n",
        "def fact_cheking(text, request, counter=0, model=o3_model, system_prompt = None):\n",
        "\n",
        "    client = OpenAI()\n",
        "\n",
        "    system_prompt = \"Ты эксперт по технологическим проектам\"\n",
        "\n",
        "    prompt = f\"Это описание коммерческой компании-производителя (есть свои собственные продукты их производства, при этом продукт - это не услуга,\\\n",
        "    продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "    под конкретного заказчика не является продуктом), соответствующее направлению {request} - ответь только одно слово 'ДА' или 'НЕТ'?\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'task: {prompt} : \"text: {text}\" '}]\n",
        "    response = client.chat.completions.create(model=model, messages=messages).choices[0].message.content\n",
        "\n",
        "    if response == 'ДА':\n",
        "      return text\n",
        "    else:\n",
        "      return 'НЕТ'\n",
        "#/fact checking"
      ],
      "metadata": {
        "id": "yZ8xUEpgEO_5"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extend request:"
      ],
      "metadata": {
        "id": "kGFuT6SpcN8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ext_requests = (request + ',' + request_synonyms(request, synonyms_number)).split(',')\n",
        "\n",
        "print('\\nОблако запросов:\\n')\n",
        "for phrase in ext_requests:\n",
        "    print('*', phrase.strip())"
      ],
      "metadata": {
        "id": "yvj0ZFBZcQcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a78f7d-93f4-4779-c0ce-cc70ddc22857"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Облако запросов:\n",
            "\n",
            "* продукт по направлению Программное обеспечение для генерации видео по промпту распознавания текста в видео и поиску по ключевым словам в видео\n",
            "* программное обеспечение для создания видео на основе текстовых подсказок и поиска по ключевым словам в видео\n",
            "* система для генерации видеоконтента по текстовым запросам и поиску по ключевым словам в видео\n",
            "* инструмент для создания видео на основе распознавания текста и поиска по ключевым словам в видео\n",
            "* приложение для генерации видео по текстовым промптам и поиску по ключевым словам в видео\n",
            "* платформа для создания видео на основе текстовых инструкций и поиска по ключевым словам в видео\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate urls:"
      ],
      "metadata": {
        "id": "461snS1wDL1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "all_urls = parallel_generators(ext_requests)\n",
        "all_urls_short = list(frozenset([get_protocol_and_domain(i) for i in all_urls]))\n",
        "\n",
        "print('\\n\\nНайденные сайты:', len(all_urls_short), '\\n')\n",
        "for i in all_urls_short:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rEHCYUC2DNz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54092686-d7cd-4ff3-deab-da15f9b8c7c1"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:25<00:00,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Найденные сайты: 45 \n",
            "\n",
            "https://www.itv.ru\n",
            "https://skillbox.ru\n",
            "https://avastudio.ru\n",
            "https://www.securno.ru\n",
            "https://www.demis.ru\n",
            "https://www.renderforest.com\n",
            "https://rvproduction.ru\n",
            "https://habr.com\n",
            "https://www.alladvertising.ru\n",
            "https://giga.chat\n",
            "https://www.it-world.ru\n",
            "https://lpmotor.ru\n",
            "https://rvigroup.ru\n",
            "https://www.tadviser.ru\n",
            "https://macroscop.com\n",
            "https://pogumax.ru\n",
            "https://www.vedita.ru\n",
            "https://fabricacontenta.ru\n",
            "https://www.dssl.ru\n",
            "https://ppc.world\n",
            "https://akool.com\n",
            "https://alti-group.ru\n",
            "https://freud-pr.ru\n",
            "https://telematica.ru\n",
            "https://humanvideo.ru\n",
            "https://pikabu.ru\n",
            "https://marketing-tech.ru\n",
            "https://trassir.ru\n",
            "https://callibri.ru\n",
            "https://www.aistudios.com\n",
            "https://www.rush-analytics.ru\n",
            "https://rosvideo.pro\n",
            "https://replay-video.ru\n",
            "https://masspers.com\n",
            "https://trends.rbc.ru\n",
            "https://www.fastvideo.ru\n",
            "https://dtf.ru\n",
            "https://www.livebusiness.ru\n",
            "https://pcnews.ru\n",
            "https://platforma-online.ru\n",
            "https://ru.wadline.com\n",
            "https://salesvideoproduction.ru\n",
            "https://t-j.ru\n",
            "https://www.unisender.com\n",
            "https://vc.ru\n",
            "CPU times: user 71.4 ms, sys: 9.16 ms, total: 80.6 ms\n",
            "Wall time: 25.2 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract summaries:"
      ],
      "metadata": {
        "id": "VNcVd9zWte9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries = [None] * len(all_urls_short)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(all_urls_short)) as executor:\n",
        "\n",
        "    futures = {executor.submit(parse_description, url): i for i, url in enumerate(all_urls_short)}\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        index = futures[future]\n",
        "        summaries[index] = future.result()\n",
        "\n",
        "print('\\n\\nЧисло саммари:', len(summaries), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gparkQ1ti-N",
        "outputId": "7672292b-73c6-4447-ba22-8a8772e04d99"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:13<00:00,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Число саммари: 45 \n",
            "\n",
            "CPU times: user 805 ms, sys: 56.1 ms, total: 861 ms\n",
            "Wall time: 13.1 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fact checking:"
      ],
      "metadata": {
        "id": "6lepxFBKIs0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries_new = [None] * len(summaries)\n",
        "fact_checking_partial = partial(fact_cheking, request=request) # patch\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(summaries)) as executor: # patch\n",
        "    summaries_new = list(tqdm(executor.map(fact_checking_partial, summaries), total=len(summaries)))\n",
        "\n",
        "try:\n",
        "  all_urls_short_new, summaries_new = zip(*[(u, t) for u, t in zip(all_urls_short, summaries_new) if not 'НЕТ' in t])\n",
        "\n",
        "  comp_names = [s.split(';', 1)[0].strip() for s in list(summaries_new)]\n",
        "  comp_descriptions =[s.split(';', 1)[1].strip() if ';' in s else '' for s in list(summaries_new)]\n",
        "\n",
        "  df_final = pd.DataFrame({'Сайт': all_urls_short_new, 'Имя компании': comp_names, 'Описание': comp_descriptions})\n",
        "  print('\\n\\n', df_final.to_markdown(index=False, tablefmt=\"grid\"))\n",
        "except:\n",
        "  print('No data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a1p64JJInAt",
        "outputId": "907785f9-ad69-466e-f1b5-ed1308b0b149"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:20<00:00,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " +------------------------------+----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Сайт                         | Имя компании   | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+==============================+================+=====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+\n",
            "| https://www.renderforest.com | Renderforest   | Renderforest — это универсальная платформа брендинга, предоставляющая инструменты на базе искусственного интеллекта для создания видео, анимаций, логотипов, веб‑сайтов, мокапов и графики. Среди технологий упоминаются AI Video Generator, AI Animation Generator, Text to Video AI, AI Website Builder и AI Logo Generator, а также REST API и SDK для NodeJS и PHP. Продукты компании включают Video Maker, Logo Maker, Website Maker, Mockup Maker, Graphic Maker и мобильные приложения для iOS и Android, применяемые в маркетинге, брендинге, обучении и создании контента. |\n",
            "+------------------------------+----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "CPU times: user 9.58 s, sys: 95.6 ms, total: 9.68 s\n",
            "Wall time: 20.8 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save to xlsx:"
      ],
      "metadata": {
        "id": "05Ee4YcJ8O5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_excel('web_agent.xlsx', index=False)\n",
        "files.download('web_agent.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FzsOTEFy8G40",
        "outputId": "5e7e794a-7d3f-4ade-8262-006af2a67328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8676b47-75e2-49b4-8c1e-ab31e1250edc\", \"web_agent.xlsx\", 8289)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JEK-UAOKmAQi",
        "ewazALFKnQ5j",
        "BMvuvqKk0cFg",
        "05Ee4YcJ8O5g"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}