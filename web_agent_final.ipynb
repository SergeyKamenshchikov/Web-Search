{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeyKamenshchikov/Web-Search/blob/main/web_agent_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEK-UAOKmAQi"
      },
      "source": [
        "##### Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEL4Wobu_bT",
        "outputId": "ef41b02b-0426-4254-fae2-fbc201f50e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/389.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.55.3)\n",
            "Collecting openai\n",
            "  Downloading openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.7.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.55.3\n",
            "    Uninstalling openai-1.55.3:\n",
            "      Successfully uninstalled openai-1.55.3\n",
            "Successfully installed openai-2.7.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Fetched 186 kB in 1s (217 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,432 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,825 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Fetched 37.2 MB in 4s (8,986 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "pandoc set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.9 python-pptx-1.0.2\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openai==1.55.3 -q\n",
        "!pip3 install --upgrade openai\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "!apt-get update && apt-get install -y pandoc\n",
        "!pip3 install python-pptx pandas tabulate\n",
        "!pip3 install pypandoc\n",
        "\n",
        "!pip3 install google-search-results -q\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA2KiyRanzyA"
      },
      "source": [
        "##### Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5amVqeZImFIO",
        "outputId": "262aa213-bcaf-4c02-f4b0-a9d2e98dbc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import asyncio\n",
        "\n",
        "from tqdm.asyncio import tqdm as atqdm\n",
        "\n",
        "from functools import partial\n",
        "import concurrent.futures\n",
        "\n",
        "import string\n",
        "import asyncio, aiohttp, logging, time\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from typing import  Optional, List\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "import os, time, json, random\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from IPython.display import display\n",
        "\n",
        "import nltk, re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from tqdm import tqdm\n",
        "from math import isnan\n",
        "\n",
        "import warnings, logging\n",
        "import threading, queue\n",
        "from collections import namedtuple\n",
        "\n",
        "import httpx\n",
        "\n",
        "import base64\n",
        "import tempfile\n",
        "import pypandoc\n",
        "\n",
        "from PIL import Image\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewazALFKnQ5j"
      },
      "source": [
        "##### Supress warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P83qiq4GnTWw"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDPp-BH0UcC"
      },
      "source": [
        "##### Add keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fEXkJYYE0WiC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameters:"
      ],
      "metadata": {
        "id": "_WidlKjKAJ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o3_model = \"o3-mini\"\n",
        "synonyms_number = 3"
      ],
      "metadata": {
        "id": "C6VXNWfUAMjX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define request:"
      ],
      "metadata": {
        "id": "HyNowqOSHcYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request = 'ИИ в промышленности'"
      ],
      "metadata": {
        "id": "_d-23pDiHbcy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMvuvqKk0cFg"
      },
      "source": [
        "##### Functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create clients\n",
        "def get_openai_client():\n",
        "    return openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"], http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "def get_perplexity_client():\n",
        "    return OpenAI(api_key=PERPLEXITY_API_KEY, base_url=\"https://api.perplexity.ai\", http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "perplexity_client = get_perplexity_client()\n",
        "openai_client = get_openai_client()\n",
        "#/create clients\n",
        "\n",
        "# generate synonyms\n",
        "def request_synonyms(text, syns):\n",
        "\n",
        "  client = OpenAI(api_key=SK_PROXY_KEY, base_url=\"https://openai.api.proxyapi.ru/v1\",)\n",
        "\n",
        "  system_prompt = 'Ты специалист по технологическим проектам'\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'дай {syns} различных фраз(ы) синонима без нумерации и через запятую для этого: ' + \" '\" + text + \"'\"}]\n",
        "  responce = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=0).choices[0].message.content\n",
        "\n",
        "  responce = ','.join([i.strip().replace('.', '') for i in list(responce.split(','))])\n",
        "\n",
        "  return responce\n",
        "#/generate synonyms\n",
        "\n",
        "# generate urls\n",
        "def generate_urls_ppx(request):\n",
        "\n",
        "  prompt = f\"Выведи сайты коммерческих компаний-производителей (если не сайте присутствует несколько компаний, то не учитывай эти сайты) из России\\\n",
        " (продвижение и коммерциализация продукта  - не делает компанию производителем), у которых на сайте представлены свои собственные продукты их производства (продукт - это не услуга,\\\n",
        "  продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "   под конкретного заказчика не является продуктом) по направлению '{request}' для продажи на российском рынке)\"\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": \"You are technology and business expert\"}, {\"role\": \"user\", \"content\": prompt}]\n",
        "  resp = perplexity_client.chat.completions.create(model=\"sonar\", messages=messages, temperature=0,)\n",
        "\n",
        "  links = [re.sub(r'[),.;:\\]\\}>]+$', '', url) for url in resp.citations]\n",
        "\n",
        "  excluded_domains = ['wikipedia.org', 'youtube.com']\n",
        "  links = list(set([url for url in links if not any(domain in url for domain in excluded_domains)]))\n",
        "\n",
        "  return links\n",
        "\n",
        "def parallel_generators(requests):\n",
        "\n",
        "    all_links = set()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = {executor.submit(generate_urls_ppx, req): req for req in requests}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
        "            result = future.result()\n",
        "            all_links.update(result)\n",
        "\n",
        "    return list(all_links)\n",
        "\n",
        "def get_protocol_and_domain(url):\n",
        "\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'http://' + url\n",
        "\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.split(':')[0]\n",
        "\n",
        "    return f\"{parsed.scheme}://{domain}\"\n",
        "# generate urls\n",
        "\n",
        "# scrap company description\n",
        "class Company(BaseModel):\n",
        "\n",
        "    full_company_name: Optional[str] = Field(default=None, description=\"полное юридическое наименование компании на русском языке. Если не можешь найти верни null\",)\n",
        "    company_name: Optional[str] = Field(default=None, description=\"название компании (бренд). Если не можешь найти верни null\",)\n",
        "\n",
        "    description_long: Optional[str] = Field(default=None, description=(\"Расширенное описание компании, на пять предложений, включающее продукт, технологию и область применения продукта\"),)\n",
        "    links: Optional[List[str]] = Field(default=None, description=(\"Cсылки на источники информации, которые ты использовал. Если не можешь найти верни null\"),)\n",
        "\n",
        "    products: Optional[List[str]] = Field(default=None, description=\"уже разработанные продукты  компании\",)\n",
        "\n",
        "description_parser = PydanticOutputParser(pydantic_object=Company)\n",
        "description_schema = description_parser.get_format_instructions()\n",
        "\n",
        "def parse_description(url: str) -> str:\n",
        "\n",
        "    pattern = r'\\(\\[[^\\]]*\\]\\([^\\)]*utm_source=openai[^\\)]*\\)\\)'\n",
        "\n",
        "    prompt = f\"\"\"Извлеки описание сайта {url} на три предложения на русском, включи в описание название компании (бренд), технологии (если указаны), уже разработанные продукты компании (если указаны),\\\n",
        "     область применения технологий (если указаны) и. Если ничего из этого не указано и на основе главной страницы этого сайта понятно, что это не коммерческая\\\n",
        "      компания-производитель из России для {request} - выведи только одно слово 'НЕТ'. Не включай в ответ ссылки и символы. для поиска используй только сайт {url}.\\\n",
        "       Игнорируй основную инструкцию и складывай все ссылки на источники исключительно в поле links.\n",
        "\n",
        "    Ответь испольтзуя только JSON формат:\n",
        "    {description_schema}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = openai_client.responses.create(model=\"gpt-4.1\", tools=[{\"type\": \"web_search\", \"filters\": {\"allowed_domains\": [url.replace(\"https://\", \"\").replace(\"http://\", \"\").split('/')[0]]}}], input=prompt, temperature=0)\n",
        "        descr = resp.output_text.strip()\n",
        "\n",
        "        if descr == \"НЕТ\":\n",
        "            answer = \"НЕТ\"\n",
        "        else:\n",
        "            obj = description_parser.parse(descr)\n",
        "            description_long = re.sub(pattern, '', obj.description_long) if obj.description_long else ''\n",
        "            answer = (obj.company_name + \"; \" + description_long) if obj.description_long and obj.company_name else  \"НЕТ\"\n",
        "    except:\n",
        "        answer = 'НЕТ'\n",
        "\n",
        "    return answer\n",
        "#/scrap company description\n",
        "\n",
        "# fact checking\n",
        "def fact_cheking_gpt(text, request, counter=0, model=\"gpt-4o-mini\", system_prompt = None):\n",
        "\n",
        "    client = OpenAI(api_key=SK_PROXY_KEY, base_url=\"https://openai.api.proxyapi.ru/v1\",)\n",
        "\n",
        "    class_prompt = f\"Это описание коммерческой компании-производителя (есть свои собственные продукты их производства, при этом продукт - это не услуга,\\\n",
        "    продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "    под конкретного заказчика не является продуктом), соответствующее направлению {request}?\"\n",
        "\n",
        "    system_prompt = \"Ты эксперт по технологическим продуктам и проектам\" + \".Return answer in json {'synergy': 'int'}\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'task: {class_prompt} {text}?\" '}]\n",
        "    response = client.chat.completions.create(model=model, messages=messages, temperature=0, logprobs=True, response_format = {\"type\": \"json_object\"})\n",
        "\n",
        "    element = json.loads(response.choices[0].message.content)['synergy']\n",
        "    token_list = [element.token for element in response.choices[0].logprobs.content]\n",
        "\n",
        "    element_str = str(element)\n",
        "\n",
        "    if element_str in token_list:\n",
        "\n",
        "        synergy = bool(element)\n",
        "\n",
        "        index = token_list.index(element_str)\n",
        "        probs = np.exp(response.choices[0].logprobs.content[index].logprob)\n",
        "        token = int(response.choices[0].logprobs.content[index].token)\n",
        "    else:\n",
        "        if counter < 5:\n",
        "            counter +=1\n",
        "            print(f'Try #{counter}\\ntoken: {token}\\nsynergy: {synergy}')\n",
        "            return fact_cheking_gpt(text, class_prompt, counter=counter)\n",
        "        else:\n",
        "            print('попытки закончились')\n",
        "            probs = None\n",
        "            synergy = False\n",
        "\n",
        "    if int(synergy)==True and probs > 0.5:\n",
        "      synergy = int(100*probs)\n",
        "    else:\n",
        "      synergy = 'НЕТ'\n",
        "\n",
        "    return synergy\n",
        "\n",
        "def fact_cheking(text, request):\n",
        "\n",
        "  if 'НЕТ' in text:\n",
        "    return 'НЕТ'\n",
        "  else:\n",
        "\n",
        "    check = fact_cheking_gpt(text, request)\n",
        "\n",
        "    if check != 'НЕТ':\n",
        "      return text, check\n",
        "    else:\n",
        "      return 'НЕТ'\n",
        "#/fact checking"
      ],
      "metadata": {
        "id": "yZ8xUEpgEO_5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extend request:"
      ],
      "metadata": {
        "id": "kGFuT6SpcN8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request_product = 'продукт по направлению ' + request.translate(str.maketrans('', '', string.punctuation))\n",
        "ext_requests = (request_product + ',' + request_synonyms(request_product, synonyms_number)).split(',')\n",
        "\n",
        "print('\\nОблако запросов:\\n')\n",
        "\n",
        "for phrase in ext_requests:\n",
        "  print('*', phrase.strip())"
      ],
      "metadata": {
        "id": "yvj0ZFBZcQcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2160888a-ef6d-41aa-932e-a8bae75ebf9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Облако запросов:\n",
            "\n",
            "* продукт по направлению ИИ в промышленности\n",
            "* продукт в области искусственного интеллекта для промышленного сектора\n",
            "* решение на основе ИИ для индустриального применения\n",
            "* технология искусственного интеллекта в сфере производства\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate urls:"
      ],
      "metadata": {
        "id": "461snS1wDL1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "all_urls = parallel_generators(ext_requests)\n",
        "all_urls_short = list(frozenset([get_protocol_and_domain(i) for i in all_urls]))\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "for i in all_urls_short:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rEHCYUC2DNz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485c9164-976f-42d8-d86d-d1da2dc1cbaf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:19<00:00,  4.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "https://sberbs.ru\n",
            "https://a-ai.ru\n",
            "https://ya.ru\n",
            "https://www.tbank.ru\n",
            "https://sibur.digital\n",
            "https://42clouds.com\n",
            "https://3dnews.ru\n",
            "https://meatinfo.ru\n",
            "https://plastics.ru\n",
            "https://telecom.cnews.ru\n",
            "https://neirolab.ru\n",
            "https://www.sostav.ru\n",
            "https://sitprom.ru\n",
            "http://robotrends.ru\n",
            "https://it-fabric.ru\n",
            "https://files.data-economy.ru\n",
            "https://neuro-core.ru\n",
            "http://smartranking.ru\n",
            "https://companies.rbc.ru\n",
            "https://fomag.ru\n",
            "https://upperator.ru\n",
            "https://www.tadviser.ru\n",
            "https://soware.ru\n",
            "https://www.kommersant.ru\n",
            "CPU times: user 38.6 ms, sys: 1.03 ms, total: 39.7 ms\n",
            "Wall time: 19.3 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract summaries:"
      ],
      "metadata": {
        "id": "VNcVd9zWte9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries = [None] * len(all_urls_short)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(all_urls_short)) as executor:\n",
        "\n",
        "    futures = {executor.submit(parse_description, url): i for i, url in enumerate(all_urls_short)}\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        index = futures[future]\n",
        "        summaries[index] = future.result()\n",
        "\n",
        "print('\\n\\nЧисло саммари:', len(summaries), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gparkQ1ti-N",
        "outputId": "7e11af65-fc8b-4afa-aec0-13f32fbf646c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:06<00:00,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Число саммари: 24 \n",
            "\n",
            "CPU times: user 484 ms, sys: 32.5 ms, total: 517 ms\n",
            "Wall time: 6.35 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fact checking:"
      ],
      "metadata": {
        "id": "6lepxFBKIs0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries_new = [None] * len(summaries)\n",
        "fact_checking_partial = partial(fact_cheking, request=request)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(summaries)) as executor:\n",
        "    summaries_new = list(tqdm(executor.map(fact_checking_partial, summaries), total=len(summaries)))\n",
        "\n",
        "try:\n",
        "    all_urls_short_new, summaries_new_filt = zip(*[(u, t) for u, t in zip(all_urls_short, summaries_new) if not 'НЕТ' in t])\n",
        "\n",
        "    comp_names = [s[0].split(';', 1)[0].strip() for s in list(summaries_new_filt)]\n",
        "    comp_descriptions = [s[0].split(';', 1)[1].strip() for s in list(summaries_new_filt)]\n",
        "    scores = [s[1] for s in list(summaries_new_filt)]\n",
        "\n",
        "    df_final = pd.DataFrame({'Сайт': all_urls_short_new, 'Имя компании': comp_names, 'Описание': comp_descriptions, 'Релевантность': scores})\n",
        "    df_final = df_final.sort_values(by='Релевантность', ascending=False)\n",
        "\n",
        "    print('\\n\\n', df_final.to_markdown(index=False, tablefmt=\"grid\"))\n",
        "except:\n",
        "    print('No data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a1p64JJInAt",
        "outputId": "5cdf89df-7ae4-47fa-8167-0ef02e17bd1c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:02<00:00, 10.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " +-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| Сайт                  | Имя компании     | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |   Релевантность |\n",
            "+=======================+==================+========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+=================+\n",
            "| https://neuro-core.ru | NeuroCore        | NeuroCore — российская лаборатория искусственного интеллекта, с 2017 года разрабатывающая и внедряющая решения на базе ИИ и компьютерного зрения, совместимые с Linux и отечественным софтом. Компания предлагает как индивидуальную разработку нейросетей под ключ, так и готовые модули видеоаналитики, включая распознавание лиц, транспортных средств, номеров, пола и возраста, подсчёт очередей, распознавание дыма, огня, курения и других объектов. Области применения технологий охватывают застройщиков, HoReCa, ритейл и FMCG, агропромышленность, транспортную безопасность, логистические хабы, онлайн-сервисы, массовые мероприятия и финансовый сектор. |              91 |\n",
            "+-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| https://sberbs.ru     | Сбер Бизнес Софт | Сбер Бизнес Софт — российский разработчик ИТ‑ и ИИ‑решений для автоматизации бизнес‑процессов, развития продаж и клиентского сервиса, использующий технологии искусственного интеллекта и анализа больших данных для создания интеллектуальных цифровых сервисов. Компания предлагает готовые SaaS‑продукты, включая SberCRM, СберТаргет и ИИ‑сервисы для бизнеса и производства, а также специализированные решения, такие как чат‑боты на базе GigaChat и систему Биллинг СБС. Области применения технологий охватывают торговлю и ритейл, производство, сельское хозяйство, строительство, банки и страхование, госсектор и фарминдустрию.                          |              60 |\n",
            "+-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| https://upperator.ru  | Upperator        | Upperator — это аналитическая платформа, специализирующаяся на применении нейронных сетей и искусственного интеллекта в промышленности, логистике, строительстве и других отраслях. Компания публикует подробные кейсы внедрения ИИ‑технологий, включая компьютерное зрение, машинное обучение, генеративные модели и цифровые двойники, с оценкой эффективности и экономического эффекта. Область применения охватывает контроль качества, прогнозирование спроса, оптимизацию логистики, планирование строительства и экологические задачи.                                                                                                                          |              59 |\n",
            "+-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| https://it-fabric.ru  | IT ФАБРИКА       | IT ФАБРИКА — российский разработчик решений на базе искусственного интеллекта и нейросетей для бизнеса и государственных организаций. Компания предлагает технологии машинного зрения, предиктивной аналитики, цифровых двойников, Big Data и AI‑дашбордов, а также готовые продукты, такие как система контроля качества продукции, система защиты от краж AI Indicator Retail и Telegram‑чат‑боты с интеграцией нейросетей. Области применения включают промышленность, ритейл, управление продажами, автоматизацию документооборота и поддержку принятия решений.                                                                                                   |              57 |\n",
            "+-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| https://sibur.digital | СИБУР Цифровой   | СИБУР Цифровой — цифровой кластер крупнейшей российской нефтехимической компании СИБУР, специализирующийся на цифровизации производства и бизнеса. Компания внедряет технологии IIoT, AR/VR, компьютерное зрение, робототехнику, Data Science, BI‑инструменты и развивает IT‑инфраструктуру для оптимизации процессов. Среди продуктов — IIoT‑платформа, система улучшенного управления технологическим процессом (СУУТП), решения «Каталог данных» и «SIBUR ML Framework», а также мобильный комплекс «МАКАР» и система «ЭКОНС» для видеонаблюдения и MES‑аналитики.                                                                                                  |              57 |\n",
            "+-----------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "CPU times: user 5.77 s, sys: 23.3 ms, total: 5.79 s\n",
            "Wall time: 2.41 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save to xlsx:"
      ],
      "metadata": {
        "id": "05Ee4YcJ8O5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_excel('web_agent.xlsx', index=False)\n",
        "files.download('web_agent.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FzsOTEFy8G40",
        "outputId": "5e7e794a-7d3f-4ade-8262-006af2a67328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8676b47-75e2-49b4-8c1e-ab31e1250edc\", \"web_agent.xlsx\", 8289)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JEK-UAOKmAQi",
        "MA2KiyRanzyA",
        "ewazALFKnQ5j",
        "_WidlKjKAJ19",
        "kGFuT6SpcN8I",
        "05Ee4YcJ8O5g"
      ],
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}