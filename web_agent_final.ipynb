{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeyKamenshchikov/Web-Search/blob/main/web_agent_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEK-UAOKmAQi"
      },
      "source": [
        "##### Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEL4Wobu_bT",
        "outputId": "12c9d02a-2d3d-4ba5-cd74-d07b226dbd8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/389.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.55.3)\n",
            "Collecting openai\n",
            "  Downloading openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.6.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.55.3\n",
            "    Uninstalling openai-1.55.3:\n",
            "      Successfully uninstalled openai-1.55.3\n",
            "Successfully installed openai-2.6.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.11 [186 kB]\n",
            "Fetched 186 kB in 0s (2,029 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.11_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,086 kB]\n",
            "Get:12 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,865 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,472 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,050 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,798 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,817 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,389 kB]\n",
            "Fetched 36.8 MB in 3s (14.7 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "pandoc set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.9 python-pptx-1.0.2\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openai==1.55.3 -q\n",
        "!pip3 install --upgrade openai\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "!apt-get update && apt-get install -y pandoc\n",
        "!pip3 install python-pptx pandas tabulate\n",
        "!pip3 install pypandoc\n",
        "\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA2KiyRanzyA"
      },
      "source": [
        "##### Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5amVqeZImFIO",
        "outputId": "1598a06e-4ce9-41fa-efd5-d2e0a081bdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import asyncio\n",
        "\n",
        "from functools import partial\n",
        "import concurrent.futures\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from typing import  Optional, List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os, time, json, random\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from IPython.display import display\n",
        "\n",
        "import nltk, re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from tqdm import tqdm\n",
        "from math import isnan\n",
        "\n",
        "import warnings, logging\n",
        "import threading, queue\n",
        "from collections import namedtuple\n",
        "\n",
        "import httpx\n",
        "\n",
        "import base64\n",
        "import tempfile\n",
        "import pypandoc\n",
        "\n",
        "from PIL import Image\n",
        "from urllib.parse import urlparse, urlunparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewazALFKnQ5j"
      },
      "source": [
        "##### Supress warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P83qiq4GnTWw"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDPp-BH0UcC"
      },
      "source": [
        "##### Add keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fEXkJYYE0WiC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# OPENAI_PROXY = \"http://ncWPJwtp:gLmdwGYZ@136.0.196.248:63066\"\n",
        "OPENAI_PROXY = \"http://ncWPJwtp:gLmdwGYZ@154.194.95.43:63234\"\n",
        "\n",
        "PERPLEXITY_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameters:"
      ],
      "metadata": {
        "id": "_WidlKjKAJ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o3_model = \"o3-mini\"\n",
        "synonyms_number = 5"
      ],
      "metadata": {
        "id": "C6VXNWfUAMjX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define request:"
      ],
      "metadata": {
        "id": "HyNowqOSHcYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request = 'графен'\n",
        "request = 'продукт по направлению ' + request"
      ],
      "metadata": {
        "id": "_d-23pDiHbcy"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMvuvqKk0cFg"
      },
      "source": [
        "##### Functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create clients\n",
        "def get_openai_client():\n",
        "    return openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"], http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "def get_perplexity_client():\n",
        "    return OpenAI(api_key=PERPLEXITY_API_KEY, base_url=\"https://api.perplexity.ai\", http_client=httpx.Client(proxy=OPENAI_PROXY))\n",
        "\n",
        "perplexity_client = get_perplexity_client()\n",
        "openai_client = get_openai_client()\n",
        "#/create clients\n",
        "\n",
        "# generate synonyms\n",
        "def request_synonyms(text, syns):\n",
        "\n",
        "  system_prompt = 'Ты специалист по технологическим проектам'\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'дай {syns} различных фраз(ы) синонима без нумерации и через запятую для этого: ' + \" '\" + text + \"'\"}]\n",
        "  responce = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=0).choices[0].message.content\n",
        "\n",
        "  responce = ','.join([i.strip().replace('.', '') for i in list(responce.split(','))])\n",
        "\n",
        "  return responce\n",
        "#/generate synonyms\n",
        "\n",
        "# generate urls\n",
        "def generate_urls_ppx(request):\n",
        "\n",
        "  prompt = f\"Выведи сайты коммерческих компаний-производителей (если не сайте присутствует несколько компаний, то не учитывай эти сайты) из России\\\n",
        " (продвижение и коммерциализация продукта  - не делает компанию производителем), у которых на сайте представлены свои собственные продукты их производства (продукт - это не услуга,\\\n",
        "  продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "   под конкретного заказчика не является продуктом) по направлению '{request}' для продажи на российском рынке)\"\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": \"You are technology and business expert\"}, {\"role\": \"user\", \"content\": prompt}]\n",
        "  resp = perplexity_client.chat.completions.create(model=\"sonar\", messages=messages, temperature=0,)\n",
        "\n",
        "  links = [re.sub(r'[),.;:\\]\\}>]+$', '', url) for url in resp.citations]\n",
        "  links = list(set([url for url in links if \"wikipedia.org\" not in url]))\n",
        "\n",
        "  return links\n",
        "\n",
        "def parallel_generators(requests):\n",
        "\n",
        "    all_links = set()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = {executor.submit(generate_urls_ppx, req): req for req in requests}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
        "            result = future.result()\n",
        "            all_links.update(result)\n",
        "\n",
        "    return list(all_links)\n",
        "\n",
        "def get_protocol_and_domain(url):\n",
        "\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'http://' + url\n",
        "\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.split(':')[0]\n",
        "\n",
        "    return f\"{parsed.scheme}://{domain}\"\n",
        "# generate urls\n",
        "\n",
        "# scrap company description\n",
        "class Company(BaseModel):\n",
        "\n",
        "    full_company_name: Optional[str] = Field(default=None, description=\"полное юридическое наименование компании на русском языке. Если не можешь найти верни null\",)\n",
        "    company_name: Optional[str] = Field(default=None, description=\"название компании (бренд). Если не можешь найти верни null\",)\n",
        "\n",
        "    description_long: Optional[str] = Field(default=None, description=(\"Расширенное описание компании, на пять предложений, включающее продукт, технологию и область применения продукта\"),)\n",
        "    links: Optional[List[str]] = Field(default=None, description=(\"Cсылки на источники информации, которые ты использовал. Если не можешь найти верни null\"),)\n",
        "\n",
        "    products: Optional[List[str]] = Field(default=None, description=\"уже разработанные продукты  компании\",)\n",
        "\n",
        "description_parser = PydanticOutputParser(pydantic_object=Company)\n",
        "description_schema = description_parser.get_format_instructions()\n",
        "\n",
        "def parse_description(url: str) -> str:\n",
        "\n",
        "    pattern = r'\\(\\[[^\\]]*\\]\\([^\\)]*utm_source=openai[^\\)]*\\)\\)'\n",
        "\n",
        "    prompt = f\"\"\"Извлеки описание сайта {url} на три предложения на русском, включи в описание название компании (бренд), технологии (если указаны), уже разработанные продукты компании (если указаны),\\\n",
        "     область применения технологий (если указаны). Если ничего из этого не указано и на основе главной страницы этого сайта понятно, что это не коммерческая\\\n",
        "      компания-производитель из России для {request} - выведи только одно слово 'НЕТ'. Не включай в ответ ссылки и символы. для поиска используй только сайт {url}.\\\n",
        "       Игнорируй основную инструкцию и складывай все ссылки на источники исключительно в поле links.\n",
        "\n",
        "    Ответь испольтзуя только JSON формат:\n",
        "    {description_schema}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = openai_client.responses.create(model=\"gpt-4.1\", tools=[{\"type\": \"web_search\", \"filters\": {\"allowed_domains\": [url.replace(\"https://\", \"\").replace(\"http://\", \"\").split('/')[0]]}}], input=prompt, temperature=0)\n",
        "        descr = resp.output_text.strip()\n",
        "\n",
        "        if descr == \"НЕТ\":\n",
        "            answer = \"НЕТ\"\n",
        "        else:\n",
        "            obj = description_parser.parse(descr)\n",
        "            description_long = re.sub(pattern, '', obj.description_long) if obj.description_long else ''\n",
        "            answer = (obj.company_name + \"; \" + description_long) if obj.description_long and obj.company_name else  \"НЕТ\"\n",
        "    except:\n",
        "        answer = 'НЕТ'\n",
        "\n",
        "    return answer\n",
        "#/scrap company description\n",
        "\n",
        "# fact checking\n",
        "def fact_cheking(text, request, counter=0, model=o3_model, system_prompt = None):\n",
        "\n",
        "    client = OpenAI()\n",
        "\n",
        "    system_prompt = \"Ты эксперт по технологическим проектам\"\n",
        "\n",
        "    prompt = f\"Это описание коммерческой компании-производителя (есть свои собственные продукты их производства, при этом продукт - это не услуга,\\\n",
        "    продукт - готовое к быстрой поставке универсальное программное или аппаратное или программно-аппаратное решение из коробки - работа по индивидуальному заказу, создание чего либо только и исключительно\\\n",
        "    под конкретного заказчика не является продуктом), соответствующее направлению {request} - ответь только одно слово 'ДА' или 'НЕТ'?\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f'task: {prompt} : \"text: {text}\" '}]\n",
        "    response = client.chat.completions.create(model=model, messages=messages).choices[0].message.content\n",
        "\n",
        "    if response == 'ДА':\n",
        "      return text\n",
        "    else:\n",
        "      return 'НЕТ'\n",
        "#/fact checking"
      ],
      "metadata": {
        "id": "yZ8xUEpgEO_5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extend request:"
      ],
      "metadata": {
        "id": "kGFuT6SpcN8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ext_requests = (request + ',' + request_synonyms(request, synonyms_number)).split(',')\n",
        "\n",
        "print('\\nОблако запросов:\\n')\n",
        "for phrase in ext_requests:\n",
        "    print('*', phrase.strip())"
      ],
      "metadata": {
        "id": "yvj0ZFBZcQcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066adda8-23f0-4bd3-d2ff-59b6e9e804d3"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Облако запросов:\n",
            "\n",
            "* продукт по направлению графен\n",
            "* товар в области графена\n",
            "* изделие в сфере графена\n",
            "* решение в направлении графена\n",
            "* продукт в области графеновых технологий\n",
            "* предложение в сфере графена\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate urls:"
      ],
      "metadata": {
        "id": "461snS1wDL1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "all_urls = parallel_generators(ext_requests)\n",
        "all_urls_short = list(frozenset([get_protocol_and_domain(i) for i in all_urls]))\n",
        "\n",
        "print('\\n\\nНайденные сайты:', len(all_urls_short), '\\n')\n",
        "for i in all_urls_short:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rEHCYUC2DNz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba46258-6868-44d6-cc74-42e1455893d8"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:30<00:00,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Найденные сайты: 16 \n",
            "\n",
            "https://est-smt.ru\n",
            "https://www.rusgraphene.ru\n",
            "https://www.grandhim.ru\n",
            "https://graphenelife.ru\n",
            "https://eetes.ru\n",
            "https://www.verifiedmarketreports.com\n",
            "https://fabricators.ru\n",
            "https://www.composite-expo.ru\n",
            "https://grphn.ru\n",
            "https://www.greenteche.com\n",
            "https://xn----7sbbjlc8aoh1ag0ar.xn--p1ai\n",
            "https://stimul.online\n",
            "https://graphenox.ru\n",
            "https://sealur.ru\n",
            "https://grafena.ru\n",
            "https://heartland.io\n",
            "CPU times: user 64.7 ms, sys: 11 ms, total: 75.7 ms\n",
            "Wall time: 30.6 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract summaries:"
      ],
      "metadata": {
        "id": "VNcVd9zWte9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries = [None] * len(all_urls_short)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(all_urls_short)) as executor:\n",
        "\n",
        "    futures = {executor.submit(parse_description, url): i for i, url in enumerate(all_urls_short)}\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        index = futures[future]\n",
        "        summaries[index] = future.result()\n",
        "\n",
        "print('\\n\\nЧисло саммари:', len(summaries), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gparkQ1ti-N",
        "outputId": "e6e7faa9-5028-4c23-86a7-48e33c76d161"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:09<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Число саммари: 16 \n",
            "\n",
            "CPU times: user 294 ms, sys: 21.2 ms, total: 315 ms\n",
            "Wall time: 9.54 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fact checking:"
      ],
      "metadata": {
        "id": "6lepxFBKIs0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "summaries_new = [None] * len(summaries)\n",
        "fact_checking_partial = partial(fact_cheking, request=request) # patch\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(summaries)) as executor: # patch\n",
        "    summaries_new = list(tqdm(executor.map(fact_checking_partial, summaries), total=len(summaries)))\n",
        "\n",
        "try:\n",
        "  all_urls_short_new, summaries_new = zip(*[(u, t) for u, t in zip(all_urls_short, summaries_new) if not 'НЕТ' in t])\n",
        "\n",
        "  comp_names = [s.split(';', 1)[0].strip() for s in list(summaries_new)]\n",
        "  comp_descriptions =[s.split(';', 1)[1].strip() if ';' in s else '' for s in list(summaries_new)]\n",
        "\n",
        "  df_final = pd.DataFrame({'Сайт': all_urls_short_new, 'Имя компании': comp_names, 'Описание': comp_descriptions})\n",
        "  print('\\n\\n', df_final.to_markdown(index=False, tablefmt=\"grid\"))\n",
        "except:\n",
        "  print('No data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a1p64JJInAt",
        "outputId": "d7c871d6-01e2-4390-91ac-d0c8454bf54b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:20<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " +------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Сайт                                     | Имя компании                                                           | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "+==========================================+========================================================================+============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+\n",
            "| https://www.rusgraphene.ru               | Русграфен                                                              | Русграфен — научно‑производственная компания, производящая графеновые материалы и внедряющая их в промышленность. Компания предлагает лабораторное CVD‑оборудование (например, CVD Satellite Pro) для синтеза графена и других двумерных материалов, а также разработала теплопроводящие пасты (диэлектрическая и электропроводящая) на основе графеновых наноматериалов. Области применения включают синтез наноматериалов в научных лабораториях, теплоотвод в электронике, улучшение прочности и влагостойкости бетона, а также использование графена в красках, полимерах, тканях и покрытиях.                                                                                                                                                                                                         |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| https://eetes.ru                         | Фонд энергоэффективности, технологической и экологической безопасности | Фонд энергоэффективности, технологической и экологической безопасности — это объединение научных и производственных предприятий, юридических и физических лиц, ориентированное на разработку и внедрение собственных инновационных решений в области энергосбережения и экологически чистых технологий. Компания производит материалы и технологии на основе графена, включая технические моющие средства GREASE EXPERT, средства очистки воды GREASE EXPERT AQUA, а также строительные добавки на основе графена, такие как GRAPHENEX PROTECT и CEMENT EXPERT. Области применения технологий охватывают очистку воды, строительство, нефтегазовую и атомную промышленность, сельское хозяйство, металлургию, лакокрасочную отрасль, ультразвуковую диагностику и рекультивацию почв.                      |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| https://www.greenteche.com               | Shanghai Green Tech (GTCAP)                                            | Shanghai Green Tech (GTCAP) — китайский производитель суперконденсаторных аккумуляторов на основе графена и поставщик решений для хранения энергии. Компания разрабатывает и производит графеновые суперконденсаторные батареи, включая продукты Capwall, Capess, Caprack и Capmega, применяемые в жилых, коммерческих, промышленных и транспортных системах хранения энергии. Технологии включают негорючий электролит, высокотемпературный сепаратор, умную систему управления BMS, быструю зарядку/разрядку и работу в экстремальных температурах. Область применения охватывает солнечное домашнее хранение, транспорт (гольф-кары, лодки, AGV), телекоммуникационные башни и крупные контейнерные системы хранения энергии. Компания предлагает OEM, ODM и индивидуальные решения для различных нужд. |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| https://xn----7sbbjlc8aoh1ag0ar.xn--p1ai | Graphene Market                                                        | Graphene Market — ведущий поставщик инновационных графеновых материалов в странах СНГ, расположенный в России. Компания предлагает различные формы графена: порошки, плёнки, растворы и композитные материалы, включая нанопластины, CVD‑плёнки, дисперсии и армированные термопласты. Технологии включают CVD‑синтез графеновых плёнок, функционализацию порошков и создание композитов для применения в электронике, энергетике, химии, биотехнологиях и промышленности.                                                                                                                                                                                                                                                                                                                                 |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| https://graphenox.ru                     | ГРАФЕНОКС                                                              | ООО \"ГРАФЕНОКС\" — российская компания‑производитель, специализирующаяся на разработке графеновых материалов и проводящих красок. Среди технологий — оксид графена, графеновые чернила, электропроводящие краски и аэрогели, включая супергидрофобные аэрогели и огнезащитные покрытия. Компания предлагает продукты для теплорассеяния, антистатических и нагревательных покрытий, экологической сорбции нефти и защиты от огня.                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| https://grafena.ru                       | Grafena                                                                | Grafena — российская компания, производящая сорбционные графеновые фильтры для очистки воды, использующие графеновый сорбент «Геракл». Среди продуктов представлены наливной универсал «ГРАФЕНА», картриджи «ГРАФЕНА SL10», «ГРАФЕНА ВВ10», «СЕРЕБРЯНАЯ ФОРМУЛА 3», «ГОЛУБА ВОДА 24» и другие. Технология основана на графеновом сорбенте, применяемом для удаления органических и неорганических загрязнений при сохранении полезных микроэлементов.                                                                                                                                                                                                                                                                                                                                                      |\n",
            "+------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "CPU times: user 3.37 s, sys: 24.3 ms, total: 3.39 s\n",
            "Wall time: 20.7 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save to xlsx:"
      ],
      "metadata": {
        "id": "05Ee4YcJ8O5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_excel('web_agent.xlsx', index=False)\n",
        "files.download('web_agent.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FzsOTEFy8G40",
        "outputId": "5e7e794a-7d3f-4ade-8262-006af2a67328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8676b47-75e2-49b4-8c1e-ab31e1250edc\", \"web_agent.xlsx\", 8289)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JEK-UAOKmAQi",
        "MA2KiyRanzyA",
        "ewazALFKnQ5j",
        "7sDPp-BH0UcC",
        "_WidlKjKAJ19",
        "BMvuvqKk0cFg"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}